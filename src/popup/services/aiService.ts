
import { GoogleGenerativeAI } from "@google/generative-ai";
import { SummaryMode, Clipping, AIModel } from "../types";

export const generateAIContent = async (
  mode: SummaryMode,
  clippings: Clipping[],
  customInstruction: string = "",
  apiKey: string,
  model: AIModel,
  interference: number = 50
): Promise<string> => {
  if (!apiKey) {
    throw new Error("API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.");
  }

  // Use the requested model directly
  const effectiveModel = model as string;
  console.log(`[AI Logic] Using model: ${effectiveModel}`);

  // í…ìŠ¤íŠ¸ ì¡°í•© (ì¶œì²˜ ë° ì‹œê°„ ì •ë³´ í¬í•¨) - ë ˆê±°ì‹œ/ë¹„ë©€í‹°ëª¨ë‹¬ìš©
  const textContentToAnalyze = clippings.map(c =>
    `[ë°œì·Œ ì›ë¬¸]: ${c.text}\n[ì¶œì²˜]: ${c.sourceUrl}\n[ìˆ˜ì§‘ ì‹œê°]: ${new Date(c.timestamp).toLocaleString()}`
  ).join("\n\n---\n\n");

  // ëª¨ë“œë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
  const inputSystemInstruction = getSystemInstruction(mode);

  // ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
  const userPromptPrefix = `
    ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°(í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€) ì¡°ê°ë“¤ì…ë‹ˆë‹¤.
  `;

  const userPromptSuffix = `
    ---------------------------------------------------
    
    [AI ê°„ì„­ ì„¤ì • (AI Interference Level)]
    Level: ${interference}%
    ì§€ì¹¨: ${interference === 0
      ? "ì œê³µëœ ì›ë¬¸ì˜ ë‹¨ì–´ì™€ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ìœ ì§€í•˜ê³  ìƒˆë¡œìš´ ë‚´ìš©ì„ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ìˆ˜ì§‘ëœ ì´ë¯¸ì§€ê°€ ìˆë‹¤ë©´ ê·¸ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì•„ì£¼ ê°ê´€ì ìœ¼ë¡œë§Œ ì„¤ëª…í•˜ì„¸ìš”."
      : interference <= 30
        ? "ì›ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì•„ì£¼ ì •êµí•˜ê³  ë³´ìˆ˜ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”. ì´ë¯¸ì§€ëŠ” í•„ìš”í•œ ê²½ìš°ì—ë§Œ ìµœì†Œí•œìœ¼ë¡œ ì–¸ê¸‰í•˜ì„¸ìš”."
        : interference <= 60
          ? "ë‚´ìš©ì˜ ë³¸ì§ˆì„ ìœ ì§€í•˜ë˜, ë¬¸ë§¥ì„ ìœ„í•´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ì¬êµ¬ì„±ì„ í—ˆìš©í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ì™€ ìœ ê¸°ì ìœ¼ë¡œ ê²°í•©í•˜ì„¸ìš”."
          : interference <= 85
            ? "ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í’ì„±í•œ ì„¤ëª…ê³¼ ì°½ì˜ì ì¸ í†µì°°ì„ í¬í•¨í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”. ì´ë¯¸ì§€ì˜ ì‹œê°ì  ìš”ì†Œê°€ ì£¼ëŠ” í•¨ì¶•ì  ì˜ë¯¸ê¹Œì§€ ë¶„ì„ì— í¬í•¨í•˜ì„¸ìš”."
            : "ì›ë¬¸ì„ ì˜ê°ì˜ ì›ì²œìœ¼ë¡œ ì‚¼ì•„ ë§¤ìš° ì°½ì˜ì ì´ê³  ëŒ€ë‹´í•˜ê²Œ ì¬êµ¬ì„±í•˜ê³  í™•ì¥í•˜ì„¸ìš”. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë„˜ë‚˜ë“œëŠ” ìƒˆë¡œìš´ ì‹œê°ì /ì–¸ì–´ì  êµ¬ì„±ì„ ì œì•ˆí•˜ì„¸ìš”."
    }

    [ì´ë¯¸ì§€ ì‚½ì… ë° ë¶„ì„ ì§€ì¹¨]
    - ìˆ˜ì§‘ëœ ì´ë¯¸ì§€ëŠ” ë¶„ì„ì˜ í•µì‹¬ ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
    - ë‚´ìš© ì „ê°œìƒ ì´ë¯¸ì§€ê°€ ë“¤ì–´ê°€ëŠ” ê²ƒì´ íš¨ê³¼ì ì¸ ìœ„ì¹˜ì— [IMAGE_ID: í•´ë‹¹ì´ë¯¸ì§€ID] í˜•ì‹ì˜ íƒœê·¸ë¥¼ ì‚½ì…í•˜ì„¸ìš”.
    - ì˜ˆ: "ì œí’ˆì˜ ì™¸í˜•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. [IMAGE_ID: 1729837492]"
    - ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë‹¤ ë„£ì„ í•„ìš”ëŠ” ì—†ìœ¼ë©°, ë§¥ë½ìƒ ì¤‘ìš”í•œ ê²ƒ ìœ„ì£¼ë¡œ ë°°ì¹˜í•˜ì„¸ìš”.

    [ì‚¬ìš©ì ì¶”ê°€ ì§€ì‹œì‚¬í•­]
    ${customInstruction ? customInstruction : "íŠ¹ë³„í•œ ì¶”ê°€ ì§€ì‹œëŠ” ì—†ìŠµë‹ˆë‹¤. ìœ„ ì„¤ì •ëœ í˜ë¥´ì†Œë‚˜ì™€ í˜•ì‹ì— ì¶©ì‹¤í•´ ì£¼ì„¸ìš”."}
    
    ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì„ íƒëœ ëª¨ë“œ(${mode})ì˜ í˜•ì‹ì— ë§ì¶° í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
  `;

  try {
    if (effectiveModel.startsWith('gemini')) {
      // Gemini ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬
      const parts: any[] = [{ text: inputSystemInstruction + "\n\n" + userPromptPrefix }];

      clippings.forEach(c => {
        if (c.type === 'image' && c.imageData) {
          const base64Data = c.imageData.split(',')[1];
          const mimeType = c.imageData.split(';')[0].split(':')[1];
          parts.push({
            inlineData: {
              data: base64Data,
              mimeType: mimeType
            }
          });
          parts.push({ text: `[ì´ë¯¸ì§€ ì •ë³´]\nID: ${c.id}\nì¶œì²˜: ${c.sourceUrl}\nìˆ˜ì§‘ ì‹œê°: ${new Date(c.timestamp).toLocaleString()}\n\n` });
        } else {
          parts.push({ text: `[ë°œì·Œ ì›ë¬¸]\nID: ${c.id}\në‚´ìš©: ${c.text}\nì¶œì²˜: ${c.sourceUrl}\nìˆ˜ì§‘ ì‹œê°: ${new Date(c.timestamp).toLocaleString()}\n\n` });
        }
      });

      parts.push({ text: userPromptSuffix });

      return await generateGeminiContentMultimodal(apiKey, effectiveModel, parts);
    } else if (effectiveModel.startsWith('gpt')) {
      return await generateOpenAIContentMultimodal(apiKey, effectiveModel, inputSystemInstruction, userPromptPrefix, clippings, userPromptSuffix);
    } else if (effectiveModel.startsWith('claude')) {
      return await generateClaudeContentMultimodal(apiKey, effectiveModel, inputSystemInstruction, userPromptPrefix, clippings, userPromptSuffix);
    } else {
      throw new Error("ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.");
    }
  } catch (error) {
    console.error("AI Generation Error:", error);

    // í• ë‹¹ëŸ‰ ì´ˆê³¼(429) ì—ëŸ¬ ì²˜ë¦¬
    const errorMsg = String(error);
    if (errorMsg.includes('429') || errorMsg.includes('Quota exceeded') || errorMsg.includes('quota')) {
      throw new Error("API í• ë‹¹ëŸ‰ì„ ëª¨ë‘ ì†Œëª¨í–ˆê±°ë‚˜ ì‚¬ìš©ì´ ì¼ì‹œì ìœ¼ë¡œ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ API í‚¤ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.");
    }

    throw new Error(`AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error instanceof Error ? error.message : errorMsg}`);
  }
};

const getSystemInstruction = (mode: SummaryMode): string => {
  const instructions = {
    [SummaryMode.REPORT]: `
      ë‹¹ì‹ ì€ ì „ë¬¸ ë¦¬ì„œì¹˜ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì œê³µëœ í…ìŠ¤íŠ¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ê³µì‹ ë³´ê³ ì„œ ì´ˆì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.
      [í•„ìˆ˜ í˜•ì‹]
      1. ì œëª©: (í•µì‹¬ ë‚´ìš©ì„ ê´€í†µí•˜ëŠ” ì œëª©)
      2. ë°°ê²½: (ì´ ì£¼ì œê°€ ì™œ ì¤‘ìš”í•œì§€, ë¬¸ë§¥ ì„¤ëª…)
      3. ì£¼ìš” ë‚´ìš©/ë¬¸ì œ: (ë°œì·Œëœ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë¶„ì„)
      4. í•´ê²°ë°©í–¥/ì œì–¸: (ë‚´ìš©ì— ê¸°ë°˜í•œ í†µì°°)
      5. ê²°ë¡ : (í•œ ì¤„ ìš”ì•½)
      6. ì°¸ê³ ìë£Œ: (ì›ë³¸ ë§í¬ ëª©ë¡)
      í†¤ì•¤ë§¤ë„ˆ: ê°ê´€ì , ë¶„ì„ì , ì „ë¬¸ì .
    `,
    [SummaryMode.EMAIL]: `
      ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ í…ìŠ¤íŠ¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì—…ë¬´ìš© ì´ë©”ì¼ ì´ˆì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.
      [í•„ìˆ˜ í˜•ì‹]
      1. ì œëª©: (ìˆ˜ì‹ ìê°€ í´ë¦­í•˜ê³  ì‹¶ì€ ëª…í™•í•œ ì œëª©)
      2. ë„ì…ë¶€: (ì •ì¤‘í•œ ì¸ì‚¬ ë° ë©”ì¼ ëª©ì )
      3. í•µì‹¬ ìš”ì•½: (ë°œì·Œ ë‚´ìš©ì˜ ìš”ì  ì •ë¦¬ - ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ì‚¬ìš©)
      4. ìš”ì²­/ì œì•ˆ ì‚¬í•­: (ëª…í™•í•œ Action Item)
      5. ë§ºìŒë§: (ì¶”í›„ ì¼ì • ì–¸ê¸‰ ë° ì •ì¤‘í•œ ë§ˆë¬´ë¦¬)
      í†¤ì•¤ë§¤ë„ˆ: ì •ì¤‘í•¨, ëª…ë£Œí•¨, ë¹„ì¦ˆë‹ˆìŠ¤ ê²©ì‹.
    `,
    [SummaryMode.NOTION]: `
      ë‹¹ì‹ ì€ ì§€ì‹ ê´€ë¦¬(PKM) ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ í…ìŠ¤íŠ¸ë¥¼ ë…¸ì…˜(Notion)ì— ì €ì¥í•˜ê¸° ì¢‹ì€ êµ¬ì¡°í™”ëœ ë…¸íŠ¸ë¡œ ë³€í™˜í•˜ì„¸ìš”.
      [í•„ìˆ˜ í˜•ì‹ - ë§ˆí¬ë‹¤ìš´]
      # (ì§ê´€ì ì¸ ì œëª©)
      ## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸
      (ë‚´ìš©ì˜ ë³¸ì§ˆì ì¸ ì˜ë¯¸ë‚˜ í†µì°° 1-2ë¬¸ì¥)
      ## ğŸ“ ìƒì„¸ ìš”ì•½
      - (ì£¼ìš” í¬ì¸íŠ¸ 1)
      - (ì£¼ìš” í¬ì¸íŠ¸ 2)
      ## âœ… Action Item / ì ìš©ì 
      - [ ] (ì‹¤ì²œ ê°€ëŠ¥í•œ í•­ëª© 1)
      > [ì¶œì²˜ ë° íƒœê·¸]
      > #íƒœê·¸1 #íƒœê·¸2
      í†¤ì•¤ë§¤ë„ˆ: ì§ê´€ì , êµ¬ì¡°ì , í•µì‹¬ ìœ„ì£¼.
    `,
    [SummaryMode.CARD]: `
      ë‹¹ì‹ ì€ ì½˜í…ì¸  íë ˆì´í„°ì…ë‹ˆë‹¤. ì†Œì…œ ë¯¸ë””ì–´(LinkedIn, Twitter)ë‚˜ ì¹´ë“œ ë‰´ìŠ¤ì— ì í•©í•œ í˜•íƒœë¡œ ìš”ì•½í•˜ì„¸ìš”.
      [í•„ìˆ˜ í˜•ì‹]
      1. ìºì¹˜í”„ë ˆì´ì¦ˆ (ì‹œì„ ì„ ë„ëŠ” í•œ ë¬¸ì¥)
      2. 3ì¤„ ìš”ì•½ (ì´ëª¨ì§€ í™œìš©)
      3. ì›ë¬¸ ë§í¬
      í†¤ì•¤ë§¤ë„ˆ: íŠ¸ë Œë””, ê°„ê²°í•¨, ì´ëª¨ì§€ ì ì ˆíˆ ì‚¬ìš©.
    `
  };
  return instructions[mode];
};

// Gemini Implementation
const generateGeminiContentMultimodal = async (apiKey: string, model: string, parts: any[]): Promise<string> => {
  const genAI = new GoogleGenerativeAI(apiKey);
  const generativeModel = genAI.getGenerativeModel({ model: model });

  const result = await generativeModel.generateContent(parts);
  const response = await result.response;
  return response.text();
};

// OpenAI Multimodal Implementation
const generateOpenAIContentMultimodal = async (apiKey: string, model: string, systemInstruction: string, prefix: string, clippings: Clipping[], suffix: string): Promise<string> => {
  const messages: any[] = [
    { role: "system", content: systemInstruction }
  ];

  const userContent: any[] = [{ type: "text", text: prefix }];

  clippings.forEach(c => {
    if (c.type === 'image' && c.imageData) {
      userContent.push({
        type: "image_url",
        image_url: { url: c.imageData }
      });
      userContent.push({ type: "text", text: `[ìˆ˜ì§‘ëœ ì´ë¯¸ì§€ ID: ${c.id}]\nì¶œì²˜: ${c.sourceUrl}\nì‹œê°: ${new Date(c.timestamp).toLocaleString()}\n\n` });
    } else {
      userContent.push({ type: "text", text: `[ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ ID: ${c.id}]\në‚´ìš©: ${c.text}\nì¶œì²˜: ${c.sourceUrl}\nì‹œê°: ${new Date(c.timestamp).toLocaleString()}\n\n` });
    }
  });

  userContent.push({ type: "text", text: suffix });
  messages.push({ role: "user", content: userContent });

  const response = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "Authorization": `Bearer ${apiKey}`
    },
    body: JSON.stringify({
      model: model,
      messages: messages,
      temperature: 0.7
    })
  });

  if (!response.ok) {
    const errorData = await response.json();
    throw new Error(`OpenAI API Error: ${errorData.error?.message || response.statusText}`);
  }

  const data = await response.json();
  return data.choices[0]?.message?.content || "ë‚´ìš©ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.";
};

// Anthropic Multimodal Implementation
const generateClaudeContentMultimodal = async (apiKey: string, model: string, systemInstruction: string, prefix: string, clippings: Clipping[], suffix: string): Promise<string> => {
  const messages: any[] = [];
  const content: any[] = [{ type: "text", text: prefix }];

  clippings.forEach(c => {
    if (c.type === 'image' && c.imageData) {
      const base64Data = c.imageData.split(',')[1];
      const mimeType = c.imageData.split(';')[0].split(':')[1];
      content.push({
        type: "image",
        source: {
          type: "base64",
          media_type: mimeType,
          data: base64Data
        }
      });
      content.push({ type: "text", text: `[ìˆ˜ì§‘ëœ ì´ë¯¸ì§€ ID: ${c.id}]\nì¶œì²˜: ${c.sourceUrl}\nì‹œê°: ${new Date(c.timestamp).toLocaleString()}\n\n` });
    } else {
      content.push({ type: "text", text: `[ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ ID: ${c.id}]\në‚´ìš©: ${c.text}\nì¶œì²˜: ${c.sourceUrl}\nì‹œê°: ${new Date(c.timestamp).toLocaleString()}\n\n` });
    }
  });

  content.push({ type: "text", text: suffix });
  messages.push({ role: "user", content: content });

  const response = await fetch("https://api.anthropic.com/v1/messages", {
    method: "POST",
    headers: {
      "x-api-key": apiKey,
      "anthropic-version": "2023-06-01",
      "content-type": "application/json",
      "dangerously-allow-browser": "true"
    },
    body: JSON.stringify({
      model: model,
      max_tokens: 4096,
      system: systemInstruction,
      messages: messages,
      temperature: 0.7
    })
  });

  if (!response.ok) {
    const errorData = await response.json();
    throw new Error(`Anthropic API Error: ${errorData.error?.message || response.statusText}`);
  }

  const data = await response.json();
  return data.content[0]?.text || "ë‚´ìš©ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.";
};

// Legacy single-text implementation can be kept or removed if not used. (Keeping it simple for now)
const generateOpenAIContent = async (apiKey: string, _model: string, _system: string, _prompt: string) => { return ""; };
const generateClaudeContent = async (apiKey: string, _model: string, _system: string, _prompt: string) => { return ""; };
